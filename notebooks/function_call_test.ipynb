{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold, content_types\n",
    "from collections.abc import Iterable\n",
    "from function_calls import final_factuality_factor_score, emotion_analyzer\n",
    "from poli_stance_function_calling import perspective_analyzer\n",
    "from google.protobuf.struct_pb2 import Struct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"API_KEY\")\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation: https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/docs/function-calling/python.ipynb, https://github.com/google-gemini/cookbook/blob/main/quickstarts/Function_calling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools = {\n",
    "#   \"functions\": [\n",
    "#       {\n",
    "#         \"name\": \"emotion_analyzer\",\n",
    "#         \"description\": \"Analyzes the emotionality and exaggeration level in a given text and returns a scaled score.\",\n",
    "#         \"parameters\": {\n",
    "#           \"type\": \"object\",\n",
    "#           \"properties\": {\n",
    "#             \"text\": {\n",
    "#               \"type\": \"string\",\n",
    "#               \"description\": \"The text content to analyze for emotional intensity and exaggeration.\"\n",
    "#             }\n",
    "#           },\n",
    "#           \"required\": [\"text\"]\n",
    "#         }\n",
    "#       },\n",
    "#       {\n",
    "#         \"name\": \"final_factuality_factor_score\",\n",
    "#         \"description\": \"Averages the microfactors from a single factuality factor. This function should be used when combining into an overall score.\",\n",
    "#         \"parameters\": {\n",
    "#             \"type\": \"object\",\n",
    "#             \"properties\": {\n",
    "#                 \"microfactor_1\": {\n",
    "#                     \"type\": \"float\",\n",
    "#                     \"description\": \"First microfactor for a factuality factor, used to help calculate the factuality factor\"\n",
    "#                 },\n",
    "#                 \"microfactor_2\": {\n",
    "#                     \"type\": \"float\",\n",
    "#                     \"description\": \"Second microfactor for a factuality factor, used to help calculate the factuality factor\"\n",
    "#                 },\n",
    "#                 \"microfactor_3\": {\n",
    "#                     \"type\": \"float\",\n",
    "#                     \"description\": \"Third microfactor for a factuality factor, used to help calculate the factuality factor\"\n",
    "#                 }\n",
    "#             },\n",
    "#             \"required\": [\"microfactor_1\", \"microfactor_2\", \"microfactor_3\"]\n",
    "#         }\n",
    "#       }\n",
    "#   ]\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment function calling part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"max_output_tokens\": 4000, # less output, means faster\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "    \"temperature\": 1, # higher temp --> more risks the model takes with choices\n",
    "    \"top_p\": 0.95, # how many tokens are considered when producing outputs\n",
    "    \"top_k\": 40, # token is selected from 40 likely tokens\n",
    "}\n",
    "\n",
    "safety_settings = {\n",
    "  HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "  HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "  HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "  HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "def tool_config_from_mode(mode: str, fns: Iterable[str] = ()):\n",
    "    \"\"\"Create a tool config with the specified function calling mode.\"\"\"\n",
    "    return content_types.to_tool_config(\n",
    "        {\"function_calling_config\": {\"mode\": mode, \"allowed_function_names\": fns}}\n",
    "    )\n",
    "\n",
    "functions = {\"final_factuality_factor_score\": final_factuality_factor_score,\n",
    "            \"emotion_analyzer\": emotion_analyzer,}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-pro-002\",\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    "    tools = functions.values(),\n",
    "    system_instruction=\"\"\"\n",
    "        You are trying to fight against misinformation by scoring different articles on their factuality factors. \n",
    "        In your responses:\n",
    "        - Use each function only once per request.\n",
    "        - Integrate the results from the function calls to generate a complete response.\n",
    "        - Do not assess an article until you are given a factuality factor to grade on.\n",
    "        - Be concise and avoid redundant function calls.    \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat = model.start_chat(enable_automatic_function_calling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = chat.send_message(\"\"\"\n",
    "# \"Shocking Discovery: Scientists Unveil a Secret That Will Change Humanity Forever!.\"\n",
    "# Given the previous sentence, rate the sentence based on the defined microfactors of the factuality factor, sensationalism, from 1 to 10, ensuring that you explain the reasoning behind each score, and feel free to use function calling:\n",
    "# 1. Sensationalism Detection: Identify instancesS of sensationalism in titles and main content.\n",
    "# 2. Emotion Analysis: Assess the writing style for excessive emotionality or exaggeration.\n",
    "# 3. Linguistic Database Comparison: Match linguistic features against databases of both trusted and untrusted sources to ascertain reliability.\n",
    "# Then, combine them into an overall score for sensationalism and explain your thought process.\n",
    "# Lastly, please phrase the overall score as 'Normal Prompting Overall sensationalism: {score}', where score is calculated via the final_factuality_factor_score function. Each function call should be used once.\n",
    "# \"\"\")\n",
    "# print(f\"Model response: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_function(function_call, functions):\n",
    "    function_name = function_call.name\n",
    "    function_args = function_call.args\n",
    "    return functions[function_name](**function_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_config = tool_config_from_mode(\"auto\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_generate_content(input):\n",
    "    response = model.generate_content(input, tool_config=tool_config)\n",
    "    parts = response.candidates[0].content.parts\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"parts\": [input]})\n",
    "    for part in parts:\n",
    "        if part.function_call:\n",
    "            result = call_function(part.function_call, functions)\n",
    "            s = Struct()\n",
    "            s.update({\"result\": result})\n",
    "            function_response = genai.protos.Part(\n",
    "                function_response=genai.protos.FunctionResponse(name=part.function_call.name, response=s)\n",
    "            )\n",
    "            messages.append({\"role\": \"model\", \"parts\": [part]})\n",
    "            messages.append({\"role\": \"user\", \"parts\": [function_response]})\n",
    "        else:\n",
    "            messages.append({\"role\": \"model\", \"parts\": [part.text]})\n",
    "                # fmt: off\n",
    "            #     {\"role\": \"user\",\n",
    "            #     \"parts\": [\"'Shocking Discovery: Scientists Unveil a Secret That Will Change Humanity Forever!.'\\\n",
    "            # Given the previous sentence, rate the sentence based on the defined microfactors of the factuality factor, sensationalism, from 1 to 10, ensuring that you explain the reasoning behind each score, and feel free to use function calling:\\\n",
    "            # 1. Sensationalism Detection: Identify instancesS of sensationalism in titles and main content.\\\n",
    "            # 2. Emotion Analysis: Assess the writing style for excessive emotionality or exaggeration.\\\n",
    "            # 3. Linguistic Database Comparison: Match linguistic features against databases of both trusted and untrusted sources to ascertain reliability.\\\n",
    "            # Then, combine them into an overall score for sensationalism and explain your thought process.\\\n",
    "            # Lastly, please phrase the overall score as 'Normal Prompting Overall sensationalism: {score}', where score is calculated via the final_factuality_factor_score function. Each function call should be used once.\"]},\n",
    "            #     {\"role\": \"model\",\n",
    "            #     \"parts\": response.candidates[0].content.parts},\n",
    "            #     {\"role\": \"user\",\n",
    "            #     \"parts\": [function_response]},\n",
    "            #     # fmt: on\n",
    "    print(messages)\n",
    "    new_response = model.generate_content(messages)\n",
    "    return new_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion_score: 5.17\n",
      "factuality_score: 8.0\n",
      "[{'role': 'user', 'parts': [\"'Shocking Discovery: Scientists Unveil a Secret That Will Change Humanity Forever!.'Given the previous sentence, rate the sentence based on the defined microfactors of the factuality factor, sensationalism, from 1 to 10, ensuring that you explain the reasoning behind each score, and feel free to use function calling:1. Sensationalism Detection: Identify instancesS of sensationalism in titles and main content.2. Emotion Analysis: Assess the writing style for excessive emotionality or exaggeration.3. Linguistic Database Comparison: Match linguistic features against databases of both trusted and untrusted sources to ascertain reliability.Then, combine them into an overall score for sensationalism and explain your thought process.Lastly, please phrase the overall score as 'Normal Prompting Overall sensationalism: {score}', where score is calculated via the final_factuality_factor_score function. Each function call should be used once.\"]}, {'role': 'model', 'parts': [function_call {\n",
      "  name: \"emotion_analyzer\"\n",
      "  args {\n",
      "    fields {\n",
      "      key: \"text\"\n",
      "      value {\n",
      "        string_value: \"Shocking Discovery: Scientists Unveil a Secret That Will Change Humanity Forever!.\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "]}, {'role': 'user', 'parts': [function_response {\n",
      "  name: \"emotion_analyzer\"\n",
      "  response {\n",
      "    fields {\n",
      "      key: \"result\"\n",
      "      value {\n",
      "        number_value: 5.17\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "]}, {'role': 'model', 'parts': [function_call {\n",
      "  name: \"final_factuality_factor_score\"\n",
      "  args {\n",
      "    fields {\n",
      "      key: \"microfactor_3\"\n",
      "      value {\n",
      "        number_value: 7\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"microfactor_2\"\n",
      "      value {\n",
      "        number_value: 8\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"microfactor_1\"\n",
      "      value {\n",
      "        number_value: 9\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "]}, {'role': 'user', 'parts': [function_response {\n",
      "  name: \"final_factuality_factor_score\"\n",
      "  response {\n",
      "    fields {\n",
      "      key: \"result\"\n",
      "      value {\n",
      "        number_value: 8\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "]}]\n"
     ]
    }
   ],
   "source": [
    "response = fc_generate_content(\"'Shocking Discovery: Scientists Unveil a Secret That Will Change Humanity Forever!.'\\\n",
    "Given the previous sentence, rate the sentence based on the defined microfactors of the factuality factor, sensationalism, from 1 to 10, ensuring that you explain the reasoning behind each score, and feel free to use function calling:\\\n",
    "1. Sensationalism Detection: Identify instancesS of sensationalism in titles and main content.\\\n",
    "2. Emotion Analysis: Assess the writing style for excessive emotionality or exaggeration.\\\n",
    "3. Linguistic Database Comparison: Match linguistic features against databases of both trusted and untrusted sources to ascertain reliability.\\\n",
    "Then, combine them into an overall score for sensationalism and explain your thought process.\\\n",
    "Lastly, please phrase the overall score as 'Normal Prompting Overall sensationalism: {score}', where score is calculated via the final_factuality_factor_score function. Each function call should be used once.\")\n",
    "# break down prompt and split into different intentions, intention: {}\n",
    "# distinguish between each chunk, each chunk has a header "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensationalism Detection: 9/10. The phrase \"Shocking Discovery\" and \"Change Humanity Forever\" are clear indicators of sensationalism. These phrases aim to instantly grab attention and create a sense of urgency, but are often not backed by substantial evidence.\n",
      "Emotion Analysis: 8/10. The analyzer returned a score of 5.17, which is slightly above average, suggesting the presence of emotionality and exaggeration. Although it doesn't employ heavily loaded emotional language, the overall tone is still designed to evoke a strong emotional response.\n",
      "Linguistic Database Comparison: 7/10. Assuming this sentence was compared against a linguistic database, and it was found to have some similarities with less reliable sources, the score of 7 reflects a moderate level of concern regarding the language used.\n",
      "\n",
      "Normal Prompting Overall sensationalism: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Political stance function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"max_output_tokens\": 4000, # less output, means faster\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "    \"temperature\": 1, # higher temp --> more risks the model takes with choices\n",
    "    \"top_p\": 0.95, # how many tokens are considered when producing outputs\n",
    "    \"top_k\": 40, # token is selected from 40 likely tokens\n",
    "}\n",
    "\n",
    "safety_settings = {\n",
    "  HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "  HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "  HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "  HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "def tool_config_from_mode(mode: str, fns: Iterable[str] = ()):\n",
    "    \"\"\"Create a tool config with the specified function calling mode.\"\"\"\n",
    "    return content_types.to_tool_config(\n",
    "        {\"function_calling_config\": {\"mode\": mode, \"allowed_function_names\": fns}}\n",
    "    )\n",
    "\n",
    "functions = {\"final_factuality_factor_score\": final_factuality_factor_score,\n",
    "            \"emotion_analyzer\": emotion_analyzer,\n",
    "            \"perspective_analyzer\": perspective_analyzer}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-pro-002\",\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    "    tools = functions.values(),\n",
    "    system_instruction=\"\"\"\n",
    "        You are trying to fight against misinformation by scoring different articles on their factuality factors. \n",
    "        In your responses:\n",
    "        - Use each function only once per request.\n",
    "        - Integrate the results from the function calls to generate a complete response.\n",
    "        - Do not assess an article until you are given a factuality factor to grade on.\n",
    "        - Be concise and avoid redundant function calls.    \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_function(function_call, functions):\n",
    "    function_name = function_call.name\n",
    "    function_args = function_call.args\n",
    "    return functions[function_name](**function_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_config = tool_config_from_mode(\"auto\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_generate_content(input):\n",
    "    response = model.generate_content(input, tool_config=tool_config)\n",
    "    parts = response.candidates[0].content.parts\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"parts\": [input]})\n",
    "    for part in parts:\n",
    "        if part.function_call:\n",
    "            result = call_function(part.function_call, functions)\n",
    "            s = Struct()\n",
    "            s.update({\"result\": result})\n",
    "            function_response = genai.protos.Part(\n",
    "                function_response=genai.protos.FunctionResponse(name=part.function_call.name, response=s)\n",
    "            )\n",
    "            messages.append({\"role\": \"model\", \"parts\": [part]})\n",
    "            messages.append({\"role\": \"user\", \"parts\": [function_response]})\n",
    "        else:\n",
    "            messages.append({\"role\": \"model\", \"parts\": [part.text]})\n",
    "                # fmt: off\n",
    "            #     {\"role\": \"user\",\n",
    "            #     \"parts\": [\"'Shocking Discovery: Scientists Unveil a Secret That Will Change Humanity Forever!.'\\\n",
    "            # Given the previous sentence, rate the sentence based on the defined microfactors of the factuality factor, sensationalism, from 1 to 10, ensuring that you explain the reasoning behind each score, and feel free to use function calling:\\\n",
    "            # 1. Sensationalism Detection: Identify instancesS of sensationalism in titles and main content.\\\n",
    "            # 2. Emotion Analysis: Assess the writing style for excessive emotionality or exaggeration.\\\n",
    "            # 3. Linguistic Database Comparison: Match linguistic features against databases of both trusted and untrusted sources to ascertain reliability.\\\n",
    "            # Then, combine them into an overall score for sensationalism and explain your thought process.\\\n",
    "            # Lastly, please phrase the overall score as 'Normal Prompting Overall sensationalism: {score}', where score is calculated via the final_factuality_factor_score function. Each function call should be used once.\"]},\n",
    "            #     {\"role\": \"model\",\n",
    "            #     \"parts\": response.candidates[0].content.parts},\n",
    "            #     {\"role\": \"user\",\n",
    "            #     \"parts\": [function_response]},\n",
    "            #     # fmt: on\n",
    "    print(messages)\n",
    "    new_response = model.generate_content(messages)\n",
    "    return new_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Resource has been exhausted (e.g. check quota).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mfc_generate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mShocking Discovery: Scientists Unveil a Secret That Will Change Humanity Forever!.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;43mGiven the previous sentence, rate the sentence based on political stance with 1 being extremely biased towards democratic and \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;43m10 being extremely biased towards conservative and 5 being neutral from a score of 1 to 10. \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;43mFirst, rate the setence on the defined microfactors of the factuality factor, sensationalism, from 1 to 10, \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;43mensuring that you explain the reasoning behind each score, and feel free to use function calling:\u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;43m1. Perspective Analysis: Identify underlying perspectives on issues or events. \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;43m2. Bias evaluation: Evaluate if the stance is consistently biased. \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;43m3. verify facts: Compare the stance against verified facts. This should be done paragraph by paragraph. \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;43mThen, combine them into an overall score for a political stance. Lastly please phrase the overall score as \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNormal Prompting Overall Stance: \u001b[39;49m\u001b[38;5;132;43;01m{score}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;43m, where score is calculated via the final_factuality_factor_score function. Each function call should be used once.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# break down prompt and split into different intentions, intention: {}\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# distinguish between each chunk, each chunk has a header \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m, in \u001b[0;36mfc_generate_content\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfc_generate_content\u001b[39m(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     parts \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcandidates[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mparts\n\u001b[0;32m      4\u001b[0m     messages \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\saman\\anaconda3\\envs\\gemini\\Lib\\site-packages\\google\\generativeai\\generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\saman\\anaconda3\\envs\\gemini\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:830\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 830\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\saman\\anaconda3\\envs\\gemini\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saman\\anaconda3\\envs\\gemini\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saman\\anaconda3\\envs\\gemini\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[1;32mc:\\Users\\saman\\anaconda3\\envs\\gemini\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[0;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    208\u001b[0m         error_list,\n\u001b[0;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    210\u001b[0m         original_timeout,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[1;32mc:\\Users\\saman\\anaconda3\\envs\\gemini\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\saman\\anaconda3\\envs\\gemini\\Lib\\site-packages\\google\\api_core\\timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saman\\anaconda3\\envs\\gemini\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mResourceExhausted\u001b[0m: 429 Resource has been exhausted (e.g. check quota)."
     ]
    }
   ],
   "source": [
    "response = fc_generate_content(\"'Shocking Discovery: Scientists Unveil a Secret That Will Change Humanity Forever!.'\\\n",
    "Given the previous sentence, rate the sentence based on political stance with 1 being extremely biased towards democratic and \\\n",
    "10 being extremely biased towards conservative and 5 being neutral from a score of 1 to 10. \\\n",
    "First, rate the setence on the defined microfactors of the factuality factor, sensationalism, from 1 to 10, \\\n",
    "ensuring that you explain the reasoning behind each score, and feel free to use function calling:\\\n",
    "1. Perspective Analysis: Identify underlying perspectives on issues or events. \\\n",
    "2. Bias evaluation: Evaluate if the stance is consistently biased. \\\n",
    "3. verify facts: Compare the stance against verified facts. This should be done paragraph by paragraph. \\\n",
    "Then, combine them into an overall score for a political stance. Lastly please phrase the overall score as 'Normal Prompting Overall Stance: {score}' \\\n",
    ", where score is calculated via the final_factuality_factor_score function. Each function call should be used once.\")\n",
    "# break down prompt and split into different intentions, intention: {}\n",
    "# distinguish between each chunk, each chunk has a header "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
